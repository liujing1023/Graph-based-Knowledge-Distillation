# Graph-based-Knowledge-Distillation
- [Graph-based-Knowledge-Distillation](#graph-knowledge-distillation)
  - [DKD](#dkd)
  - [GKD](#gkd)
  - [Self-KD](#self-kd)




## DKD
|Method|Title|Link|Time|
|:---:|:---:|:---:|---:|
|IEP |Interpretable Embedding Procedure Knowledge Transfer via Stacked Principal Component Analysis and Graph Neural Network |[Paper](https://arxiv.org/pdf/2104.13561.pdf) |2021AAAI  |
|HKD |Distilling Holistic Knowledge with Graph Neural Networks |[Paper](https://openaccess.thecvf.com/content/ICCV2021/papers/Zhou_Distilling_Holistic_Knowledge_With_Graph_Neural_Networks_ICCV_2021_paper.pdf) |2021ICCV|
|CAG |Context-Aware Graph Inference with Knowledge Distillation for Visual Dialog |[Paper](https://pubmed.ncbi.nlm.nih.gov/34061738/) |2021TPAMI |
|DKWISL |Distilling Knowledge from Well-Informed Soft Labels for Neural Relation Extraction |[Paper](https://ojs.aaai.org//index.php/AAAI/article/view/6509) |2020AAAI |
|KTG |Knowledge Transfer Graph for Deep Collaborative Learning  |[Paper](https://arxiv.org/pdf/1909.04286.pdf) |2019ACCV |
|MHGD |Graph-based Knowledge Distillation by Multi-head Attention Network |[Paper](https://arxiv.org/pdf/1907.02226.pdf) |2019BMVC  |
|IRG |Knowledge Distillation via Instance Relationship Graph |[Paper](https://ieeexplore.ieee.org/document/8953802) |2019CVPR |
|DGCN |Binarized Collaborative Filtering with Distilling Graph Convolutional Networks |[Paper](https://arxiv.org/pdf/1906.01829.pdf) |2019IJCAI |
|GKD |Deep Geometric Knowledge Distillation with Graphs |[Paper](https://ieeexplore.ieee.org/document/9053986) |2019ICASSP |
|SPG |Spatio-Temporal Graph for Video Captioning With Knowledge Distillation |[Paper](https://openaccess.thecvf.com/content_CVPR_2020/papers/Pan_Spatio-Temporal_Graph_for_Video_Captioning_With_Knowledge_Distillation_CVPR_2020_paper.pdf) | 2020CVPR|
|MorsE |Meta-Knowledge Transfer for Inductive Knowledge Graph Embedding |[Paper](https://arxiv.org/pdf/2110.14170.pdf) |2022SIGIR |
|GCLN |Dark Reciprocal-Rank: Teacher-to-student Knowledge Transfer from Self-localization Model to Graph-convolutional Neural Network |[Paper](https://ieeexplore.ieee.org/abstract/document/9561158) |2021 ICRA |
|DOD |Deep Structured Instance Graph for Distilling Object Detectors |[Paper](https://openaccess.thecvf.com/content/ICCV2021/papers/Chen_Deep_Structured_Instance_Graph_for_Distilling_Object_Detectors_ICCV_2021_paper.pdf) |2021 ICCV|
|BAF |Better and Faster: Knowledge Transfer from Multiple Self-supervised Learning Tasks via Graph Distillation for Video Classification |[Paper](https://arxiv.org/pdf/1804.10069.pdf) |2018 IJCAI |
|LAD |Language Graph Distillation for Low-Resource Machine Translation |[Paper](https://arxiv.org/pdf/1908.06258.pdf) | 2019|
|GD |Graph Distillation for Action Detection with Privileged Modalities |[Paper](https://openaccess.thecvf.com/content_ECCV_2018/papers/Zelun_Luo_Graph_Distillation_for_ECCV_2018_paper.pdf) |2018 ECCV |
|PosVAE |Multi-label Zero-shot Classification by Learning to Transfer from External Knowledge |[Paper](https://arxiv.org/pdf/2007.15610.pdf) |2020 BMVC|
|GCMT |Graph Consistency based Mean-Teaching for Unsupervised Domain Adaptive Person Re-Identification |[Paper](https://arxiv.org/pdf/2105.04776.pdf) |2021 IJCAI |
|DistilE |DistilE: Distiling Knowledge Graph Embeddings for Faster and Cheaper Reasoning |[Paper](https://arxiv.org/pdf/2009.05912.pdf) |2022 WSDM|
|GraSSNet |Saliency Prediction with External Knowledge |[Paper](https://ieeexplore.ieee.org/document/9423113) |2021 WACV |
|LSN |Learning student networks via feature embedding |[Paper](https://ieeexplore.ieee.org/document/9007474) |2020 IEEE TNNLS |
|IntRA-KD |Inter-Region Affinity Distillation for Road Marking Segmentation |[Paper](https://ieeexplore.ieee.org/document/9156309) |2020 CVPR |
|RKD | Relational knowledge distillation |[Paper](https://ieeexplore.ieee.org/document/8954416) |2019 CVPR|
|CC |Correlation congruence for knowledge distillation |[Paper](https://ieeexplore.ieee.org/document/9009071) |2019 ICCV |
|SPKD|Similarity-preserving knowledge distillation |[Paper](https://ieeexplore.ieee.org/document/9010328) |2019 ICCV |
|KDExplainer |KDExplainer: A Task-oriented Attention Model for Explaining Knowledge Distillation |[Paper](https://arxiv.org/abs/2105.04181) |2021 IJCAI |
|TDD |Tree-like Decision Distillation |[Paper](https://openaccess.thecvf.com/content/CVPR2021/papers/Song_Tree-Like_Decision_Distillation_CVPR_2021_paper.pdf) |2021 CVPR |
|DualDE |DualDE: Dually Distilling Knowledge Graph Embedding for Faster and Cheaper Reasoning |[Paper](https://www.zhuanzhi.ai/paper/12c6a095c207373b5e207d3375290234) |2022 WSDM |
|KCAN |Conditional Graph Attention Networks for Distilling and Refining Knowledge Graphs in Recommendation |[Paper](https://dl.acm.org/doi/10.1145/3459637.3482331) |2021CIKM |
|HKDIFM |Heterogeneous Knowledge Distillation using Information Flow Modeling |[Paper](https://ieeexplore.ieee.org/document/9157683) |2020 CVPR |


## GKD
|Method|Title|Link|Time|
|:---:|:---:|:---:|:---:|
|GFKD| Graph-Free Knowledge Distillation for Graph Neural Networksâˆ— |[Paper](https://arxiv.org/pdf/2105.07519.pdf)|[Code](https://github.com/Xiang-Deng-DL/GFKD) |2021 IJCAI |
|LWC-KD |Graph Structure Aware Contrastive Knowledge Distillation for Incremental Learning in Recommender Systems |[Paper](https://dl.acm.org/doi/10.1145/3459637.3482117) | 2021CIKM|
|EGAD |EGAD: Evolving Graph Representation Learning with Self-Attention and Knowledge Distillation for Live Video Streaming Events |[Paper](https://ieeexplore.ieee.org/document/9378219) |2020IEEE International Conference on Big Data|
|GRL |Graph Representation Learning via Multi-task Knowledge Distillation |[Paper](https://arxiv.org/pdf/1911.05700.pdf) |2019 NeurIPS Workshop |
|GFL |Graph Few-shot Learning via Knowledge Transfer |[Paper](https://arxiv.org/pdf/1910.03053.pdf) |2019AAAI|
|HGKT |Heterogeneous Graph-based Knowledge Transfer for Generalized Zero-shot Learning |[Paper](https://ieeexplore.ieee.org/document/9412524) |2019ICPR |
|AGNN |Amalgamating Knowledge from Heterogeneous Graph Neural Networks |[Paper](https://ieeexplore.ieee.org/document/9577636) |2021CVPR |
|CPF |Extract the Knowledge of Graph Neural Networks and Go Beyond it: An Effective Knowledge Distillation Framework |[Paper](https://dl.acm.org/doi/10.1145/3442381.3450068) |2021 WWW|
|LSP |Distilling Knowledge from Graph Convolutional Networks |[Paper](https://ieeexplore.ieee.org/document/9156492) |2020CVPR|
|GKD |GKD:Semi-supervised Graph Knowledge Distillation for Graph-Independent Inference |[Paper](https://arxiv.org/abs/2104.03597) | |2021 MICCAI |
|scGCN |scGCN is a graph convolutional networks algorithm for knowledge transfer in single cell omics |[Paper](https://www.nature.com/articles/s41467-021-24172-y.pdf) |2021 Nature |
|MetaHG |Distilling Meta Knowledge on Heterogeneous Graph for Illicit Drug Trafficker Detection on Social Media |[Paper](https://papers.nips.cc/paper/2021/file/e234e195f3789f05483378c397db1cb5-Paper.pdf) | 2021 NeurIPS|
|Cold Brew | Cold Brew: Distilling Graph Node Representations with Incomplete or Missing Neighborhoods|[Paper](https://arxiv.org/pdf/2111.04840.pdf) |2021 ICLR |
|PGD |Privileged Graph Distillation for Cold Start Recommendation |[Paper](https://arxiv.org/pdf/2105.14975.pdf) |2021 SIGIR |
|GLNN | Graph-less Neural Networks: Teaching Old MLPs New Tricks via Distillation|[Paper](https://arxiv.org/pdf/2110.08727.pdf) |2022ICLR |
|Distill2Vec |Distill2Vec: Dynamic Graph Representation Learning with Knowledge Distillation |[Paper](https://ieeexplore.ieee.org/document/9381315) |2020 ASONAM |
|MT -GCN |Mutual Teaching for Graph Convolutional Networks |[Paper](https://arxiv.org/pdf/2009.00952.pdf) |2021 FGCS |
|RDD |Reliable Data Distillation on Graph Convolutional Network |[Paper](https://dl.acm.org/doi/10.1145/3318464.3389706) |2020SIGMOD |
|TinyGNN|TinyGNN: Learning efficient graph neural networks |[Paper](https://dl.acm.org/doi/pdf/10.1145/3394486.3403236) |2020 KDD |
|GLocalKD|Deep Graph-level Anomaly Detection by Glocal Knowledge Distillation |[Paper](https://arxiv.org/abs/2112.10063) |2022 WSDM |
|OAD |Online Adversarial Distillation for Graph Neural Networks |[Paper](https://arxiv.org/abs/2112.13966) |2021 |
|SCR |SCR: Training Graph Neural Networks with Consistency Regularization |[Paper](https://arxiv.org/abs/2112.04319v2) |2021 |
|ROD|ROD: Reception-aware Online Distillation for Sparse Graphs |[Paper](https://dl.acm.org/doi/pdf/10.1145/3447548.3467221) |2021 KDD|
|EGNN |EGNN: Constructing explainable graph neural networks via knowledge distillation |[Paper](https://www.sciencedirect.com/science/article/abs/pii/S0950705122001289) |2022 KBS|
|CKD |Collaborative Knowledge Distillation for Heterogeneous Information Network Embedding |[Paper](https://zhoushengisnoob.github.io/papers/WWW2022.pdf) |2022 WWW|
|G-CRD |On Representation Knowledge Distillation for Graph Neural Networks |[Paper](https://arxiv.org/pdf/2111.04964.pdf) | 2021|
|BGNN |Binary Graph Neural Networks |[Paper](https://ieeexplore.ieee.org/document/9578443) |2021 CVPR|
|EGSC |Slow Learning and Fast Inference: Efficient Graph Similarity Computation via Knowledge Distillation|[Paper](https://papers.nips.cc/paper/2021/file/75fc093c0ee742f6dddaa13fff98f104-Paper.pdf) |2021 NIPS |
|HSKDM |A graph neural network-based node classification model on class-imbalanced graph data |[Paper](https://www.sciencedirect.com/science/article/abs/pii/S0950705122002374?via%3Dihub) | 2022 KBS|
|MustaD | Compressing deep graph convolution network with multi-staged knowledge distillation|[Paper](https://pdfs.semanticscholar.org/1069/51f925eaf30eee7ba738c9b89e665cfb2f22.pdf?_ga=2.172565539.160323435.1657536794-2050460118.1606810570) |2021 PloS one |


## Self-KD
|Method|Title|Link|Time|
|:---:|:---:|:---:|:---:|
|LinkDist |Distilling Self-Knowledge From Contrastive Links to Classify Graph Nodes Without Passing Messages |[Paper](https://arxiv.org/pdf/2106.08541.pdf) |2021 |
|IGSD |Iterative Graph Self-distillation |[Paper](https://arxiv.org/pdf/2010.12609.pdf) |2020 The Workshop on Self-Supervised Learning for the Web|
|GNN-SD |On Self-Distilling Graph Neural Network |[Paper](https://arxiv.org/pdf/2011.02255.pdf) |2020 IJCAI|
|SDSS|Multi-task Self-distillation for Graph-based Semi-Supervised Learning |[Paper](https://arxiv.org/pdf/2112.01174.pdf) |2021|
|SAIL |SAIL: Self-Augmented Graph Contrastive Learning |[Paper](https://arxiv.org/pdf/2009.00934.pdf) |2022 AAAI |



